{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dfTrain and dfTest\n",
    "import pandas as pd\n",
    "dfTrain = pd.read_csv(\"dfTrain.csv\")\n",
    "dfTest = pd.read_csv(\"dfTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Message-ID:',\n",
       " 'Date:',\n",
       " 'From:',\n",
       " 'To:',\n",
       " 'Subject:',\n",
       " 'Mime-Version:',\n",
       " 'Content-Type:',\n",
       " 'Content-Transfer-Encoding:',\n",
       " 'X-From:',\n",
       " 'X-To:',\n",
       " 'X-cc:',\n",
       " 'X-bcc:',\n",
       " 'X-Folder:',\n",
       " 'X-Origin:',\n",
       " 'X-FileName:']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove unnecessary lines in the email, so we remove lines that starts with smth. We create a csv file that does that\n",
    "dfRemoveStartsWithRow = pd.read_csv(\"startsWith.csv\")\n",
    "#list_dfRemoveStart = dfRemoveStartsWithRow['Starting'].tolist()\n",
    "#list_dfRemoveStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/22245171/how-to-lowercase-a-pandas-dataframe-string-column-if-it-has-missing-values\n",
    "# change everything in the df to lower case if they are str\n",
    "dfTrain = dfTrain.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "dfTest = dfTest.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "dfRemoveStartsWithRow = dfRemoveStartsWithRow.applymap(lambda s:s.lower() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use NLTK for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from itertools import chain\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pickle\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def categories_and_length(df):\n",
    "    uniqueCategories = df.Area.unique()\n",
    "    print(len(uniqueCategories))\n",
    "    \n",
    "    for category in uniqueCategories:\n",
    "        dfCat = df[df.Area== category]\n",
    "        print('category: {} , len {}'.format(category, len(dfCat)))\n",
    "        \n",
    "def processDfRemoveStopWords(df, dataType):\n",
    "    '''\n",
    "    df use dfTrain and dfTest dataset\n",
    "    dataType use \"TEST\" or \"TRAIN\" used when saving file name\n",
    "    '''\n",
    "    df_list_SR = df['SR Details'].tolist()\n",
    "    \n",
    "    tupAppend = []\n",
    "    for rows in range(0, len(df)):\n",
    "        perrow = df_list_SR[rows]\n",
    "        splitString = str(perrow).split(\"\\r\\n\")\n",
    "        \n",
    "        wordsWOStopWords = []\n",
    "        for word in splitString:\n",
    "            tupAppend.append((rows, word))\n",
    "    print(tupAppend[0:10])\n",
    "    \n",
    "    # convert the tuples into a df to easily access\n",
    "    dfTuple = pd.DataFrame(tupAppend, columns = ['CellIdx','Sentence'])\n",
    "    \n",
    "    # drop empty \"Sentence\"\n",
    "    dfTuple = dfTuple[dfTuple['Sentence']!=\"\"]\n",
    "    \n",
    "    # iterate over each row\n",
    "    listOfIdxToDrop = []\n",
    "    # we want the index as well\n",
    "    for row in dfTuple.iterrows():\n",
    "        sentence = row[1].Sentence\n",
    "        Idx = row[0]\n",
    "        #print(\"sentence : {}\".format(sentence))\n",
    "        \n",
    "        for token in dfRemoveStartsWithRow.Starting:\n",
    "            if sentence.startswith(token):\n",
    "                listOfIdxToDrop.append(Idx)\n",
    "    # time to drop the relevant rows that starts with the words in dfRemoveStartsWithRow\n",
    "    print('Len before dropping: {}'.format(len(dfTuple)))\n",
    "    dfTupleDrop = dfTuple.drop(listOfIdxToDrop)\n",
    "    print('Len after dropping : {}'.format(len(dfTupleDrop)))\n",
    "    \n",
    "    # keeping cell idx, merge all rows with same cellIdx\n",
    "    # some of the sentences may have been removed and hence whole email gone\n",
    "    # combine rows with same CellIdx into one long sentence\n",
    "    numUniIdx = dfTupleDrop.CellIdx.unique()\n",
    "    \n",
    "    # first sort the CellIdx in running order\n",
    "    dfTupleSort = dfTupleDrop.sort_values(by = \"CellIdx\")\n",
    "    dfTupleSort2Col = dfTupleSort.reset_index(drop = True)\n",
    "    \n",
    "    # Merge sentences if they belong to same index\n",
    "    dfOut = dfTupleSort2Col.groupby('CellIdx')['Sentence'].apply(lambda x:\"{%s}\" % ', '.join(x))\n",
    "    \n",
    "    # convert series to dataframe\n",
    "    dfout = dfOut.to_frame()\n",
    "    fnToSave = 'output/mergedSentencesBeforeToken_{}.csv'.format(dataType)\n",
    "    dfout.to_csv(fnToSave)\n",
    "    \n",
    "    # create index columnname for the df dataframe\n",
    "    df['new_id'] = df.index\n",
    "    \n",
    "    # to make the sentences into list of sentence and now we only need cell ordering\n",
    "    # read the CSV output from above, which is tokenised to sentences by CellIdx\n",
    "    dfout = pd.read_csv(fnToSave)\n",
    "    \n",
    "    # final DF that we will work on NLTK\n",
    "    df_3Col = pd.merge(dfout, df, how ='inner', left_on ='CellIdx', right_on = 'new_id')\n",
    "    \n",
    "    ## send df_3Col to a csvfile\n",
    "    #finalEmailFn = 'output/finalEmailArea_{}.csv'.format(dataType)\n",
    "    #df_3Col.to_csv(finalEmailFn)\n",
    "    \n",
    "    # tokenise into sentence\n",
    "    # at this point it is ok to mess up the words and sentences orders    \n",
    "    dfEmailArea = df_3Col\n",
    "    \n",
    "    # tokenise into words\n",
    "    sents1 = []\n",
    "    for text in dfEmailArea['Sentence']:\n",
    "        sents1.append(sent_tokenize(text))\n",
    "    sentsUnlist1 = [' '.join(item) for item in sents1]\n",
    "    sents1 = sentsUnlist1\n",
    "    words1 = [word_tokenize(sent) for sent in sents1]\n",
    "    dfEmailArea = dfEmailArea.assign(tokenised = words1)\n",
    "    \n",
    "    # remove stopwords\n",
    "    customStopWords = set(stopwords.words('english')+list(punctuation))\n",
    "    wordsWOStopwords = []\n",
    "    for text in dfEmailArea['tokenised']:\n",
    "        text = \" \".join(text)\n",
    "        wordsWOStopwords.append([word for word in word_tokenize(text.lower()) if word not in customStopWords]) ##\n",
    "        \n",
    "    dfEmailArea = dfEmailArea.assign(removeStopwords = wordsWOStopwords)\n",
    "    wordsWOStopwords = list(chain(*wordsWOStopwords)) # unlist a list of lists\n",
    "    \n",
    "    # Stemming words / Lemmatizing\n",
    "    # stemming the column \"removeStopwords\" which is the final column after removing Stopwords\n",
    "    st = LancasterStemmer()\n",
    "    # as opposed to stemming, we use LEMMATIZER\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    LemWord = []\n",
    "    for sentence in dfEmailArea['removeStopwords']:\n",
    "        sentence = \" \".join(sentence)\n",
    "        lemWords = [lemmatizer.lemmatize(word) for word in word_tokenize(sentence)]\n",
    "        LemWord.append([stem for stem in lemWords])\n",
    "    dfEmailArea = dfEmailArea.assign(lemmatizedWords = LemWord)\n",
    "    \n",
    "    # merge each row of \"Sentence\" back in 1 sentence instead of list\n",
    "    lenDF = len(dfEmailArea)\n",
    "    listMergeAll = []\n",
    "    for i in range(0,lenDF):\n",
    "        lemWorded = dfEmailArea.lemmatizedWords.iloc[i]\n",
    "        mergestr = \" \".join(lemWorded)\n",
    "        listMergeAll.append(mergestr)\n",
    "        \n",
    "    dfEmailArea = dfEmailArea.assign(mergeAll = listMergeAll)\n",
    "    \n",
    "    return dfEmailArea\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'message-id: <15464986.1075855378456.javamail.evans@thyme>'), (0, 'date: fri, 4 may 2001 13:51:00 -0700 (pdt)'), (0, 'from: phillip.allen@enron.com'), (0, 'to: john.lavorato@enron.com'), (0, 'subject: re:'), (0, 'mime-version: 1.0'), (0, 'content-type: text/plain; charset=us-ascii'), (0, 'content-transfer-encoding: 7bit'), (0, 'x-from: phillip k allen'), (0, 'x-to: john j lavorato <john j lavorato/enron@enronxgate@enron>')]\n",
      "Len before dropping: 35261\n",
      "Len after dropping : 19354\n",
      "[(0, 'message-id: <3555083.1075855675348.javamail.evans@thyme>'), (0, 'date: wed, 9 aug 2000 06:28:00 -0700 (pdt)'), (0, 'from: phillip.allen@enron.com'), (0, 'to: stagecoachmama@hotmail.com'), (0, 'subject: '), (0, 'mime-version: 1.0'), (0, 'content-type: text/plain; charset=us-ascii'), (0, 'content-transfer-encoding: 7bit'), (0, 'x-from: phillip k allen'), (0, 'x-to: stagecoachmama@hotmail.com')]\n",
      "Len before dropping: 9741\n",
      "Len after dropping : 6433\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for dfTrain and dfTest\n",
    "dftrain = processDfRemoveStopWords(dfTrain, 'TRAIN')\n",
    "dftest = processDfRemoveStopWords(dfTest, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest\n",
    "# export dftrain later i need to import for plotting decision tree\n",
    "dftrain.to_csv(\"output/dftrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "X = dftrain['mergeAll']\n",
    "Y = dftrain['Area']\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove sparse terms\n",
    "X1 = remove_zero_tf_idf(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 is the new X\n",
    "# Y is stil Y\n",
    "\n",
    "# now we fit the model decision tree\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.kernel_approximation import AdditiveChi2Sampler\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# modelling\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('chi2sampler',AdditiveChi2Sampler(sample_steps=2)),\n",
    "    ('clf',tree.DecisionTreeClassifier()),\n",
    "    \n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "    'clf__alpha':(1e-4),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'))])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the pipeline\n",
    "text_clf.fit(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "savename_text_clf = 'output/fittedmodel_decisionTree.pickle'\n",
    "with open(savename_text_clf,'wb') as fp:\n",
    "    pickle.dump(text_clf,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test(train test) set for prediction\n",
    "predicted = text_clf.predict(dftest['mergeAll'])\n",
    "dftest = dftest.assign(predicted = predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score  support   pred       AUC\n",
      "cas           0.260417  0.431034  0.324675     58.0   96.0  0.478056\n",
      "cash          0.309091  0.250000  0.276423     68.0   55.0  0.478936\n",
      "trade         0.480000  0.320000  0.384000     75.0   50.0  0.561905\n",
      "avg / total   0.358818  0.328358  0.330487    201.0  201.0  0.500483\n"
     ]
    }
   ],
   "source": [
    "# dftest is the testing on part of the train set in reality we do that, usually ppl random select 30% from train set\n",
    "# as the testing set. In my case, i just take a separate set frm the whole set since i didnt want to use the super big dataset\n",
    "# for this demonstration on github\n",
    "\n",
    "# report the AUC metrics with F1 scores\n",
    "from roc_auc_f1_score_metrics import class_report\n",
    "y_score = text_clf.predict_proba(dftest['mergeAll'])\n",
    "\n",
    "report_with_auc = class_report(\n",
    "    y_true = dftest.Area,\n",
    "    y_pred = text_clf.predict(dftest['mergeAll']),\n",
    "    y_score = y_score\n",
    ")\n",
    "print(report_with_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the metric to a csv file for keep\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "report_name = 'output/f1_Score_ROC_decisionTree.csv'\n",
    "\n",
    "# constructing the dataframe\n",
    "labels = unique_labels(dftest.Area, predicted)\n",
    "precision,recall,fscore, support = metrics.precision_recall_fscore_support(dftest.Area, predicted, labels= labels)\n",
    "\n",
    "result_pd = pd.DataFrame({\n",
    "    \"class\": labels,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1-score\":fscore,\n",
    "    \"support\":support\n",
    "})\n",
    "\n",
    "# save file\n",
    "result_pd.to_csv(report_name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load back the train set data\n",
    "import pandas as pd\n",
    "from sklearn.tree import export_graphviz\n",
    "import subprocess\n",
    "\n",
    "X = dftrain['mergeAll']\n",
    "Y = dftrain['Area']\n",
    "\n",
    "text_clf.fit(X,Y)\n",
    "# check is fitted\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "def vectorize_data():\n",
    "    try:\n",
    "        check_is_fitted(text_clf,'tree_','the tfidf vector is not fitted')\n",
    "    except NotFittedError :\n",
    "        print(\"Not fitted yet\")\n",
    "vectorize_data()\n",
    "# export as a dot file\n",
    "export_graphviz(text_clf, out_file = 'output/decisionTree.dot',\n",
    "               class_names = Y,\n",
    "               rounded =True, proportion= False,\n",
    "               filled = True)\n",
    "# convert to png\n",
    "subprocess.call(['dot','-Tpng','output/decisionTree.dot','-o','DecisionTreeEmails.png','-gdpi=600'])\n",
    "# display in python\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (14,18))\n",
    "plt.imshow(plt.imread('DecisionTreeEmails.png'))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# here and below are test codes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# make corpus from X\n",
    "corpus = []\n",
    "for row in X:\n",
    "    corpus.append(row)\n",
    "\n",
    "# make corpus from Y\n",
    "# corpusY = []\n",
    "# for row in Y:\n",
    "#     corpusY.append(row)\n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer    \n",
    "# vectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=3,)\n",
    "X = vectorizer.fit_transform(corpus) # no need to transform y corpus\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# if our matrix is Sparse matrix (email may cause sparse matrix since a lot of words)\n",
    "#  min_tfidf is a threshold for defining the \"new\" 0\n",
    "import numpy as np\n",
    "def remove_zero_tf_idf(Xtr, min_tfidf = 0.04):\n",
    "    D = Xtr.toarray() # convert to dense if you want\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis = 0) # find features that are 0 in all documents (emails)\n",
    "    D = np.delete(D, np.where(tfidf_means == 0)[0], axis = 1) # delete them from the matrix\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
